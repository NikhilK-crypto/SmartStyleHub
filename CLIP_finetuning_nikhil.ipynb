{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b14aa913-436b-439d-b1d5-87719b9c05ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries \n",
    "import pandas as pd \n",
    "import numpy as np # importing libraries \n",
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel, CLIPTokenizer\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd11c9d9-4903-415b-a2d9-42bc3c452e0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>prod_name</th>\n",
       "      <th>product_type_name</th>\n",
       "      <th>product_group_name</th>\n",
       "      <th>graphical_appearance_name</th>\n",
       "      <th>colour_group_name</th>\n",
       "      <th>department_name</th>\n",
       "      <th>index_name</th>\n",
       "      <th>index_group_name</th>\n",
       "      <th>section_name</th>\n",
       "      <th>garment_group_name</th>\n",
       "      <th>detail_desc</th>\n",
       "      <th>gender</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>108775015</td>\n",
       "      <td>Strap top</td>\n",
       "      <td>Vest top</td>\n",
       "      <td>Garment Upper body</td>\n",
       "      <td>Solid</td>\n",
       "      <td>Black</td>\n",
       "      <td>Jersey Basic</td>\n",
       "      <td>Ladieswear</td>\n",
       "      <td>Ladieswear</td>\n",
       "      <td>Womens Everyday Basics</td>\n",
       "      <td>Jersey Basic</td>\n",
       "      <td>Jersey top with narrow shoulder straps.</td>\n",
       "      <td>womens</td>\n",
       "      <td>womens Solid Black Vest top, Jersey top with n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>108775044</td>\n",
       "      <td>Strap top</td>\n",
       "      <td>Vest top</td>\n",
       "      <td>Garment Upper body</td>\n",
       "      <td>Solid</td>\n",
       "      <td>White</td>\n",
       "      <td>Jersey Basic</td>\n",
       "      <td>Ladieswear</td>\n",
       "      <td>Ladieswear</td>\n",
       "      <td>Womens Everyday Basics</td>\n",
       "      <td>Jersey Basic</td>\n",
       "      <td>Jersey top with narrow shoulder straps.</td>\n",
       "      <td>womens</td>\n",
       "      <td>womens Solid White Vest top, Jersey top with n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>108775051</td>\n",
       "      <td>Strap top (1)</td>\n",
       "      <td>Vest top</td>\n",
       "      <td>Garment Upper body</td>\n",
       "      <td>Stripe</td>\n",
       "      <td>Off White</td>\n",
       "      <td>Jersey Basic</td>\n",
       "      <td>Ladieswear</td>\n",
       "      <td>Ladieswear</td>\n",
       "      <td>Womens Everyday Basics</td>\n",
       "      <td>Jersey Basic</td>\n",
       "      <td>Jersey top with narrow shoulder straps.</td>\n",
       "      <td>womens</td>\n",
       "      <td>womens Stripe Off White Vest top, Jersey top w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>111586001</td>\n",
       "      <td>Shape Up 30 den 1p Tights</td>\n",
       "      <td>Leggings</td>\n",
       "      <td>Garment Lower body</td>\n",
       "      <td>Solid</td>\n",
       "      <td>Black</td>\n",
       "      <td>Tights basic</td>\n",
       "      <td>Lingeries/Tights</td>\n",
       "      <td>Ladieswear</td>\n",
       "      <td>Womens Nightwear, Socks &amp; Tigh</td>\n",
       "      <td>Socks and Tights</td>\n",
       "      <td>Tights with built-in support to lift the botto...</td>\n",
       "      <td>womens</td>\n",
       "      <td>womens Solid Black Leggings, Tights with built...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>116379047</td>\n",
       "      <td>Frugan longsleeve</td>\n",
       "      <td>Top</td>\n",
       "      <td>Garment Upper body</td>\n",
       "      <td>Solid</td>\n",
       "      <td>Dark Blue</td>\n",
       "      <td>Jersey Basic</td>\n",
       "      <td>Ladieswear</td>\n",
       "      <td>Ladieswear</td>\n",
       "      <td>Womens Everyday Basics</td>\n",
       "      <td>Jersey Basic</td>\n",
       "      <td>Fitted top in soft stretch jersey with a wide ...</td>\n",
       "      <td>womens</td>\n",
       "      <td>womens Solid Dark Blue Top, Fitted top in soft...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id                  prod_name product_type_name  \\\n",
       "0   108775015                  Strap top          Vest top   \n",
       "1   108775044                  Strap top          Vest top   \n",
       "2   108775051              Strap top (1)          Vest top   \n",
       "3   111586001  Shape Up 30 den 1p Tights          Leggings   \n",
       "4   116379047          Frugan longsleeve               Top   \n",
       "\n",
       "   product_group_name graphical_appearance_name colour_group_name  \\\n",
       "0  Garment Upper body                     Solid             Black   \n",
       "1  Garment Upper body                     Solid             White   \n",
       "2  Garment Upper body                    Stripe         Off White   \n",
       "3  Garment Lower body                     Solid             Black   \n",
       "4  Garment Upper body                     Solid         Dark Blue   \n",
       "\n",
       "  department_name        index_name index_group_name  \\\n",
       "0    Jersey Basic        Ladieswear       Ladieswear   \n",
       "1    Jersey Basic        Ladieswear       Ladieswear   \n",
       "2    Jersey Basic        Ladieswear       Ladieswear   \n",
       "3    Tights basic  Lingeries/Tights       Ladieswear   \n",
       "4    Jersey Basic        Ladieswear       Ladieswear   \n",
       "\n",
       "                     section_name garment_group_name  \\\n",
       "0          Womens Everyday Basics       Jersey Basic   \n",
       "1          Womens Everyday Basics       Jersey Basic   \n",
       "2          Womens Everyday Basics       Jersey Basic   \n",
       "3  Womens Nightwear, Socks & Tigh   Socks and Tights   \n",
       "4          Womens Everyday Basics       Jersey Basic   \n",
       "\n",
       "                                         detail_desc  gender  \\\n",
       "0            Jersey top with narrow shoulder straps.  womens   \n",
       "1            Jersey top with narrow shoulder straps.  womens   \n",
       "2            Jersey top with narrow shoulder straps.  womens   \n",
       "3  Tights with built-in support to lift the botto...  womens   \n",
       "4  Fitted top in soft stretch jersey with a wide ...  womens   \n",
       "\n",
       "                                             caption  \n",
       "0  womens Solid Black Vest top, Jersey top with n...  \n",
       "1  womens Solid White Vest top, Jersey top with n...  \n",
       "2  womens Stripe Off White Vest top, Jersey top w...  \n",
       "3  womens Solid Black Leggings, Tights with built...  \n",
       "4  womens Solid Dark Blue Top, Fitted top in soft...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('selected_articles.csv')\n",
    "df.drop(columns=['Unnamed: 0'],inplace=True,axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7cabf59e-970e-49ef-bee4-8442bb7c5510",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['article_id', 'prod_name', 'product_type_name', 'product_group_name',\n",
       "       'graphical_appearance_name', 'colour_group_name', 'department_name',\n",
       "       'index_name', 'index_group_name', 'section_name', 'garment_group_name',\n",
       "       'detail_desc', 'gender', 'caption'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e8df03-c2b2-45ab-949d-f491aa10c6ca",
   "metadata": {},
   "source": [
    "# Taking a Sample dataset to check #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0ecfe53-094e-45fc-9087-4bb1c7bcca9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a10a9f7-1fff-47a6-bab3-75749f66fb8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>108775015</td>\n",
       "      <td>womens Solid Black Vest top, Jersey top with n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>108775044</td>\n",
       "      <td>womens Solid White Vest top, Jersey top with n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>108775051</td>\n",
       "      <td>womens Stripe Off White Vest top, Jersey top w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>111586001</td>\n",
       "      <td>womens Solid Black Leggings, Tights with built...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id                                            caption\n",
       "0   108775015  womens Solid Black Vest top, Jersey top with n...\n",
       "1   108775044  womens Solid White Vest top, Jersey top with n...\n",
       "2   108775051  womens Stripe Off White Vest top, Jersey top w...\n",
       "3   111586001  womens Solid Black Leggings, Tights with built..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample = df.iloc[:,[0,-1]]\n",
    "df_sample.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b59bf858-f50e-4c4c-8e19-250dd02eb722",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9c/xy4p20v91hv95n64yflzlj4c0000gn/T/ipykernel_77559/4080146942.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_sample['article_id'] = df_sample['article_id'].astype(str)\n",
      "/var/folders/9c/xy4p20v91hv95n64yflzlj4c0000gn/T/ipykernel_77559/4080146942.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_sample['article_id']=[\"0\"+x for x in df_sample['article_id']]\n"
     ]
    }
   ],
   "source": [
    "df_sample['article_id'] = df_sample['article_id'].astype(str)\n",
    "df_sample['article_id']=[\"0\"+x for x in df_sample['article_id']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2158a6-b8a1-4e35-a041-31f591c33fa6",
   "metadata": {},
   "source": [
    "## Text and Image Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd358945-13a3-4f5a-af88-97b08b4a342b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4f23387-9860-490b-bfa2-4adbb86b4d1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb2b4d92-78a4-4059-bce0-bf712863e2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class image_title_dataset():\n",
    "    def __init__(self, text_paths, list_image_path):\n",
    "        self.text_paths = text_paths\n",
    "        self.list_image_path = list_image_path\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.list_image_path[idx], self.text_paths[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f63f88c-1760-40d0-bd0a-48e4469d87f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/nikhilkudupudi/Projects - School & Work/Capstone/SmartStyleHub/selected_images1/0108775015.jpg'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'/Users/nikhilkudupudi/Projects - School & Work/Capstone/SmartStyleHub/selected_images1/'+ df_sample.iloc[0,0]+'.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9b41108-8699-468d-8a63-35769139138e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9c/xy4p20v91hv95n64yflzlj4c0000gn/T/ipykernel_77559/260621178.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_sample['article_id'] = df_sample['article_id'].apply(lambda x : '/Users/nikhilkudupudi/Projects - School & Work/Capstone/SmartStyleHub/selected_images1/'+ x+'.jpg')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/Users/nikhilkudupudi/Projects - School &amp; Work...</td>\n",
       "      <td>womens Solid Black Vest top, Jersey top with n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/Users/nikhilkudupudi/Projects - School &amp; Work...</td>\n",
       "      <td>womens Solid White Vest top, Jersey top with n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/Users/nikhilkudupudi/Projects - School &amp; Work...</td>\n",
       "      <td>womens Stripe Off White Vest top, Jersey top w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/Users/nikhilkudupudi/Projects - School &amp; Work...</td>\n",
       "      <td>womens Solid Black Leggings, Tights with built...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/Users/nikhilkudupudi/Projects - School &amp; Work...</td>\n",
       "      <td>womens Solid Dark Blue Top, Fitted top in soft...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          article_id  \\\n",
       "0  /Users/nikhilkudupudi/Projects - School & Work...   \n",
       "1  /Users/nikhilkudupudi/Projects - School & Work...   \n",
       "2  /Users/nikhilkudupudi/Projects - School & Work...   \n",
       "3  /Users/nikhilkudupudi/Projects - School & Work...   \n",
       "4  /Users/nikhilkudupudi/Projects - School & Work...   \n",
       "\n",
       "                                             caption  \n",
       "0  womens Solid Black Vest top, Jersey top with n...  \n",
       "1  womens Solid White Vest top, Jersey top with n...  \n",
       "2  womens Stripe Off White Vest top, Jersey top w...  \n",
       "3  womens Solid Black Leggings, Tights with built...  \n",
       "4  womens Solid Dark Blue Top, Fitted top in soft...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample['article_id'] = df_sample['article_id'].apply(lambda x : '/Users/nikhilkudupudi/Projects - School & Work/Capstone/SmartStyleHub/selected_images1/'+ x+'.jpg')\n",
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8618a7ce-9383-4eab-9aec-3c5400f02daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6e9cebf1-731c-4c67-8070-75bf14378c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test = train_test_split(df_sample, test_size=0.2, random_state= 42)\n",
    "train.reset_index(inplace= True,drop= True)\n",
    "test.reset_index(inplace= True,drop= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "61506a35-6dd9-42b7-a3dd-aa08e2cc8c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44265\n",
      "11067\n"
     ]
    }
   ],
   "source": [
    "print(len(train))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4a9731d0-d697-4ade-aae9-1e1859858329",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(image_title_dataset(train['caption'],train['article_id']), batch_size = 32, shuffle= True)\n",
    "val_dataloader = DataLoader(image_title_dataset(test['caption'],test['article_id']), batch_size= 32,shuffle= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0bc69ca4-a36c-45f8-ab2c-598046eef0da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>article_id</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>/Users/nikhilkudupudi/Projects - School &amp; Work...</td>\n",
       "      <td>mens Melange Dark Grey Sweater, Long-sleeved s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26</td>\n",
       "      <td>/Users/nikhilkudupudi/Projects - School &amp; Work...</td>\n",
       "      <td>womens Solid Black Leggings, Semi shiny stay-u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27</td>\n",
       "      <td>/Users/nikhilkudupudi/Projects - School &amp; Work...</td>\n",
       "      <td>mens Solid Black Trousers, Running trousers in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28</td>\n",
       "      <td>/Users/nikhilkudupudi/Projects - School &amp; Work...</td>\n",
       "      <td>mens Solid Dark Grey Trousers, Running trouser...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29</td>\n",
       "      <td>/Users/nikhilkudupudi/Projects - School &amp; Work...</td>\n",
       "      <td>womens Solid Beige Leggings, Matt tights with ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                         article_id  \\\n",
       "0     25  /Users/nikhilkudupudi/Projects - School & Work...   \n",
       "1     26  /Users/nikhilkudupudi/Projects - School & Work...   \n",
       "2     27  /Users/nikhilkudupudi/Projects - School & Work...   \n",
       "3     28  /Users/nikhilkudupudi/Projects - School & Work...   \n",
       "4     29  /Users/nikhilkudupudi/Projects - School & Work...   \n",
       "\n",
       "                                             caption  \n",
       "0  mens Melange Dark Grey Sweater, Long-sleeved s...  \n",
       "1  womens Solid Black Leggings, Semi shiny stay-u...  \n",
       "2  mens Solid Black Trousers, Running trousers in...  \n",
       "3  mens Solid Dark Grey Trousers, Running trouser...  \n",
       "4  womens Solid Beige Leggings, Matt tights with ...  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ae8e5b1c-400b-46be-a40b-3e78f92deb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_models_to_fp32(model): \n",
    "    for p in model.parameters(): \n",
    "        p.data = p.data.float() \n",
    "        p.grad.data = p.grad.data.float() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ddd0c3b4-0035-41c7-af2a-b01578c047ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install clip\n",
    "import clip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8215304a-a315-4a7b-92ff-111648a2f6eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0/10, Loss: 3.1953:   0%|                | 3/1384 [00:06<52:11,  2.27s/it]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/nikhilkudupudi/Projects - School & Work/Capstone/SmartStyleHub/selected_images1/0578319001.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     11\u001b[0m images,texts \u001b[38;5;241m=\u001b[39m batch \n\u001b[0;32m---> 12\u001b[0m images \u001b[38;5;241m=\u001b[39m [Image\u001b[38;5;241m.\u001b[39mopen(img_path) \u001b[38;5;28;01mfor\u001b[39;00m img_path \u001b[38;5;129;01min\u001b[39;00m images]\n\u001b[1;32m     13\u001b[0m inputs \u001b[38;5;241m=\u001b[39m processor(text\u001b[38;5;241m=\u001b[39mtexts, images\u001b[38;5;241m=\u001b[39mimages, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, max_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m77\u001b[39m)\n\u001b[1;32m     14\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {key: value\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems()}\n",
      "Cell \u001b[0;32mIn[36], line 12\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      9\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     11\u001b[0m images,texts \u001b[38;5;241m=\u001b[39m batch \n\u001b[0;32m---> 12\u001b[0m images \u001b[38;5;241m=\u001b[39m [Image\u001b[38;5;241m.\u001b[39mopen(img_path) \u001b[38;5;28;01mfor\u001b[39;00m img_path \u001b[38;5;129;01min\u001b[39;00m images]\n\u001b[1;32m     13\u001b[0m inputs \u001b[38;5;241m=\u001b[39m processor(text\u001b[38;5;241m=\u001b[39mtexts, images\u001b[38;5;241m=\u001b[39mimages, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, max_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m77\u001b[39m)\n\u001b[1;32m     14\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {key: value\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems()}\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_env/lib/python3.11/site-packages/PIL/Image.py:3218\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3215\u001b[0m     filename \u001b[38;5;241m=\u001b[39m fp\n\u001b[1;32m   3217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[0;32m-> 3218\u001b[0m     fp \u001b[38;5;241m=\u001b[39m builtins\u001b[38;5;241m.\u001b[39mopen(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   3219\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   3221\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/nikhilkudupudi/Projects - School & Work/Capstone/SmartStyleHub/selected_images1/0578319001.jpg'"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5,betas=(0.9,0.98),eps=1e-6,weight_decay=0.2)\n",
    "loss_img = nn.CrossEntropyLoss()\n",
    "loss_txt = nn.CrossEntropyLoss()\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    pbar = tqdm(train_dataloader, total=len(train_dataloader))\n",
    "    for batch in pbar:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        images,texts = batch \n",
    "        images = [Image.open(img_path) for img_path in images]\n",
    "        inputs = processor(text=texts, images=images, return_tensors=\"pt\", padding=True, truncation= True, max_length = 77)\n",
    "        inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits_per_image, logits_per_text = outputs.logits_per_image, outputs.logits_per_text\n",
    "        \n",
    "        # Compute loss\n",
    "        ground_truth = torch.arange(len(images),dtype=torch.long,device=device)\n",
    "        total_loss = (loss_img(logits_per_image,ground_truth) + loss_txt(logits_per_text,ground_truth))/2\n",
    "        logits_per_image, logits_per_text = outputs.logits_per_image, outputs.logits_per_text\n",
    "        \n",
    "        # Backward pass\n",
    "        total_loss.backward()\n",
    "        if device == \"cpu\":\n",
    "            optimizer.step()\n",
    "        else : \n",
    "            convert_models_to_fp32(model)\n",
    "            optimizer.step()\n",
    "\n",
    "        \n",
    "        pbar.set_description(f\"Epoch {epoch}/{num_epochs}, Loss: {total_loss.item():.4f}\")\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():  # No gradients needed for validation\n",
    "        for batch in val_dataloader:\n",
    "            images, texts = batch\n",
    "            images = [Image.open(img_path) for img_path in images]\n",
    "            inputs = processor(text=texts, images=images, return_tensors=\"pt\", padding=True, truncation= True, max_length = 77)\n",
    "            inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            logits_per_image, logits_per_text = outputs.logits_per_image, outputs.logits_per_text\n",
    "\n",
    "            ground_truth = torch.arange(len(images), dtype=torch.long, device=device)\n",
    "            val_loss = (loss_img(logits_per_image, ground_truth) + loss_txt(logits_per_text, ground_truth)) / 2\n",
    "            total_val_loss += val_loss.item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "    print(f\"Epoch {epoch}/{num_epochs}, Validation Loss: {avg_val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6076103-4048-4275-bd55-577e4e6b22f6",
   "metadata": {},
   "source": [
    "# testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "31210c02-7df0-4b07-8fa6-bb756ec2386a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0/10, Loss: 0.3488: 100%|███████████████████| 3/3 [00:03<00:00,  1.24s/it]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_env/lib/python3.11/site-packages/pandas/core/indexes/range.py:414\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 414\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_range\u001b[38;5;241m.\u001b[39mindex(new_key)\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[0;31mValueError\u001b[0m: 3 is not in range",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m total_val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():  \u001b[38;5;66;03m# No gradients needed for validation\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m val_dataloader:\n\u001b[1;32m     30\u001b[0m         images, texts \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m     31\u001b[0m         images \u001b[38;5;241m=\u001b[39m [Image\u001b[38;5;241m.\u001b[39mopen(img_path) \u001b[38;5;28;01mfor\u001b[39;00m img_path \u001b[38;5;129;01min\u001b[39;00m images]\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_env/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_env/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[10], line 10\u001b[0m, in \u001b[0;36mimage_title_dataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlist_image_path[idx], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_paths[idx]\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_env/lib/python3.11/site-packages/pandas/core/series.py:1040\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m-> 1040\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_value(key)\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[1;32m   1043\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[1;32m   1044\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_env/lib/python3.11/site-packages/pandas/core/series.py:1156\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1153\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[1;32m   1155\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1156\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mget_loc(label)\n\u001b[1;32m   1158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[1;32m   1159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_env/lib/python3.11/site-packages/pandas/core/indexes/range.py:416\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_range\u001b[38;5;241m.\u001b[39mindex(new_key)\n\u001b[1;32m    415\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 416\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Hashable):\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 3"
     ]
    }
   ],
   "source": [
    "# Assuming val_dataloader is defined and contains the validation dataset\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()  # Set model to training mode\n",
    "    pbar = tqdm(train_dataloader, total=len(train_dataloader))\n",
    "    for batch in pbar:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        images, texts = batch\n",
    "        images = [Image.open(img_path) for img_path in images]\n",
    "        inputs = processor(text=texts, images=images, return_tensors=\"pt\", padding=True)\n",
    "        inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        logits_per_image, logits_per_text = outputs.logits_per_image, outputs.logits_per_text\n",
    "\n",
    "        ground_truth = torch.arange(len(images), dtype=torch.long, device=device)\n",
    "        total_loss = (loss_img(logits_per_image, ground_truth) + loss_txt(logits_per_text, ground_truth)) / 2\n",
    "\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        pbar.set_description(f\"Epoch {epoch}/{num_epochs}, Loss: {total_loss.item():.4f}\")\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():  # No gradients needed for validation\n",
    "        for batch in val_dataloader:\n",
    "            images, texts = batch\n",
    "            images = [Image.open(img_path) for img_path in images]\n",
    "            inputs = processor(text=texts, images=images, return_tensors=\"pt\", padding=True)\n",
    "            inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            logits_per_image, logits_per_text = outputs.logits_per_image, outputs.logits_per_text\n",
    "\n",
    "            ground_truth = torch.arange(len(images), dtype=torch.long, device=device)\n",
    "            val_loss = (loss_img(logits_per_image, ground_truth) + loss_txt(logits_per_text, ground_truth)) / 2\n",
    "            total_val_loss += val_loss.item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "    print(f\"Epoch {epoch}/{num_epochs}, Validation Loss: {avg_val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ea1c9b-12c3-4992-aa47-3b05d408ca73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_env]",
   "language": "python",
   "name": "conda-env-pytorch_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

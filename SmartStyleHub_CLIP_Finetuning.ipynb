{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQlGzg7dDENB"
      },
      "outputs": [],
      "source": [
        "#!pip install transformers\n",
        "\n",
        "#!pip install torch\n",
        "\n",
        "#!pip install torch==1.13.1\n",
        "\n",
        "#!pip install datasets\n",
        "\n",
        "#!pip install clip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCOa2BNoDENC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import skimage\n",
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import IPython.display\n",
        "import matplotlib.pyplot as plt\n",
        "from datasets import load_dataset\n",
        "from collections import OrderedDict\n",
        "from transformers import CLIPProcessor, CLIPModel, CLIPTokenizer\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import clip\n",
        "import zipfile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dzw_1QxDEND"
      },
      "source": [
        "Load the csv file containing selected articles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TqDIgXrxDEND"
      },
      "outputs": [],
      "source": [
        "articles=pd.read_csv('selected_articles.csv',index_col=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "576cfs2UDENE"
      },
      "outputs": [],
      "source": [
        "# Adding the missing 0 to the article id and converting it to string as per the original dataset\n",
        "articles['article_id'] = articles['article_id'].astype(str)\n",
        "articles['article_id']=[\"0\"+x for x in articles['article_id']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5AiYQ3VZDENE"
      },
      "outputs": [],
      "source": [
        "df = articles[['article_id','caption']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8iQqGPWKDENE"
      },
      "source": [
        "Get the image path corresponding to each article id from zipped folders. We're adding the paths to the dataframe in a separate column.\n",
        "This will be used later to load the images during model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_AaBacmCDENE"
      },
      "outputs": [],
      "source": [
        "z1 = zipfile.ZipFile('H&M_dataset/selected_images_1.zip')\n",
        "files_in_zip_1 = z1.namelist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HQg3O8YSDENE"
      },
      "outputs": [],
      "source": [
        "id_to_path_mapping = {file.split('/')[1].split('.')[0]: file for file in files_in_zip_1}\n",
        "df['image_path'] = df['article_id'].map(id_to_path_mapping)\n",
        "\n",
        "del id_to_path_mapping"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remove missing values"
      ],
      "metadata": {
        "id": "J7OALZUBFCSO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8VA8lEFDENF"
      },
      "outputs": [],
      "source": [
        "df = df.dropna()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AxQAtXODENG"
      },
      "source": [
        "Splitting into train, validation and test datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9v51wyHKDENG"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(df['image_path'], df['caption'], test_size=0.2, random_state=1)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1)\n",
        "\n",
        "train_data = pd.DataFrame({'image_path':X_train.values, 'caption': y_train.values})\n",
        "val_data = pd.DataFrame({'image_path':X_val.values, 'caption': y_val.values})\n",
        "test_data = pd.DataFrame({'image_path':X_test.values, 'caption': y_test.values})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nDgQDZXDENG"
      },
      "source": [
        "Here we define a few functions and a class to load the CLIP model and prepare the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQO9hfw7DENH"
      },
      "outputs": [],
      "source": [
        "# Function to get CLIP model, processor and tokenizer\n",
        "def get_model_info(model_ID, device):\n",
        "\n",
        "    model = CLIPModel.from_pretrained(model_ID).to(device)\n",
        "    processor = CLIPProcessor.from_pretrained(model_ID)\n",
        "    tokenizer = CLIPTokenizer.from_pretrained(model_ID)\n",
        "\n",
        "    return model, processor, tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to convert model's parameters to FP32 format\n",
        "def convert_models_to_fp32(model):\n",
        "\n",
        "    for p in model.parameters():\n",
        "        p.data = p.data.float()\n",
        "        p.grad.data = p.grad.data.float()"
      ],
      "metadata": {
        "id": "LOYt30ifHafF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EDU08n7FDENF"
      },
      "outputs": [],
      "source": [
        "# Function to extract images from the given image path and store them as PIL objects\n",
        "def get_image(image_path):\n",
        "    if image_path in z1.namelist():\n",
        "        image = Image.open(BytesIO(z1.read(image_path))).convert(\"RGB\")\n",
        "\n",
        "    return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tvI5OHpbDENH"
      },
      "outputs": [],
      "source": [
        "# Defining a class to prepare dataset for batch processing\n",
        "class image_title_dataset():\n",
        "    def __init__(self, list_image_path,list_txt):\n",
        "\n",
        "        self.image_path = list_image_path\n",
        "        self.title  = list_txt\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.title)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        return self.image_path[idx], self.title[idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYcGul_WDENH"
      },
      "source": [
        "Prepare the dataset and create batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n9VnBKzKDENH"
      },
      "outputs": [],
      "source": [
        "list_image_path_train = []\n",
        "list_txt_train = []\n",
        "list_image_path_val = []\n",
        "list_txt_val = []\n",
        "\n",
        "for index,item in train_data.iterrows():\n",
        "    # Taking first 2000 image caption pairs from train data\n",
        "    if index==2000:\n",
        "        break\n",
        "    img_path = item['image_path']\n",
        "    caption = item['caption'][:77]\n",
        "    list_image_path_train.append(img_path)\n",
        "    list_txt_train.append(caption)\n",
        "\n",
        "for index,item in val_data.iterrows():\n",
        "  # Taking first 1000 image caption pairs from validation data\n",
        "    if index==1000:\n",
        "        break\n",
        "    img_path = item['image_path']\n",
        "    caption = item['caption'][:77]\n",
        "    list_image_path_val.append(img_path)\n",
        "    list_txt_val.append(caption)\n",
        "\n",
        "\n",
        "train_dataset = image_title_dataset(list_image_path_train, list_txt_train)\n",
        "val_dataset = image_title_dataset(list_image_path_val, list_txt_val)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=200, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=100, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLxwFwBZDENH"
      },
      "source": [
        "Load CLIP model and set model parameters"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model_ID = \"openai/clip-vit-base-patch32\"\n",
        "\n",
        "model, processor, tokenizer = get_model_info(model_ID, device)\n",
        "\n",
        "if device == \"cpu\":\n",
        "    model.float()\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5,betas=(0.9,0.98),eps=1e-6,weight_decay=0.2) # the lr is smaller, more safe for fine tuning to new dataset\n",
        "\n",
        "loss_img = nn.CrossEntropyLoss()\n",
        "loss_txt = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "3JNDAHT9HVF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k670ObZqDENH"
      },
      "source": [
        "Model finetuning on train data and evaluation on validation data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OOb13rxlDENH",
        "outputId": "e502aebe-1780-4270-d3b7-97a8cf3e397c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1 - Training: 100%|██████████| 10/10 [18:44<00:00, 112.41s/it]\n",
            "Epoch 1 - Validation: 100%|██████████| 10/10 [08:41<00:00, 52.13s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 - Train Loss: 3.4947, Validation Loss: 2.2920\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2 - Training: 100%|██████████| 10/10 [18:10<00:00, 109.04s/it]\n",
            "Epoch 2 - Validation: 100%|██████████| 10/10 [08:48<00:00, 52.84s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 - Train Loss: 2.3198, Validation Loss: 1.4726\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3 - Training: 100%|██████████| 10/10 [18:11<00:00, 109.11s/it]\n",
            "Epoch 3 - Validation: 100%|██████████| 10/10 [08:37<00:00, 51.73s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 - Train Loss: 1.5557, Validation Loss: 1.1873\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 3\n",
        "train_loss_per_epoch=[]\n",
        "val_loss_per_epoch=[]\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    # Finetuning on train data\n",
        "    train_losses=0\n",
        "\n",
        "    with tqdm(total=len(train_dataloader), desc=f'Epoch {epoch + 1} - Training') as pbar_train:\n",
        "\n",
        "        # Processing for each train batch\n",
        "        for images, texts in train_dataloader:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Load images from image paths\n",
        "            image_list=[get_image(img) for img in images]\n",
        "\n",
        "            # Encode the images and texts using CLIP's processor\n",
        "            inputs = processor(text=texts, images=image_list, return_tensors=\"pt\", padding=True)\n",
        "            inputs=inputs.to(device)\n",
        "\n",
        "            # Get image and text embeddings using CLIP model\n",
        "            outputs = model(**inputs)\n",
        "\n",
        "            # Get similarity scores between the image-text embeddings\n",
        "            logits_per_image, logits_per_text = outputs.logits_per_image, outputs.logits_per_text\n",
        "\n",
        "            # Compute loss\n",
        "            ground_truth = torch.arange(len(images),dtype=torch.long,device=device)\n",
        "            total_loss = (loss_img(logits_per_image,ground_truth) + loss_txt(logits_per_text,ground_truth))/2\n",
        "\n",
        "            # Perform backward pass\n",
        "            total_loss.backward()\n",
        "            if device == \"cpu\":\n",
        "                optimizer.step()\n",
        "\n",
        "            else :\n",
        "                convert_models_to_fp32(model)\n",
        "                optimizer.step()\n",
        "                #clip.model.convert_weights(model)\n",
        "\n",
        "            train_losses += total_loss.item()\n",
        "            pbar_train.update(1)\n",
        "\n",
        "        # Get final train loss for the epoch\n",
        "        final_train_loss=train_losses/len(train_dataloader)\n",
        "\n",
        "\n",
        "    # Evaluating on validation data - same as above but without any parameter updates\n",
        "    val_losses=0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        with tqdm(total=len(val_dataloader), desc=f'Epoch {epoch + 1} - Validation') as pbar_val:\n",
        "            for images, texts in val_dataloader:\n",
        "\n",
        "                image_list=[get_image(img) for img in images]\n",
        "                inputs = processor(text=texts, images=image_list, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "                inputs=inputs.to(device)\n",
        "                outputs = model(**inputs)\n",
        "                logits_per_image, logits_per_text = outputs.logits_per_image, outputs.logits_per_text\n",
        "\n",
        "                ground_truth = torch.arange(len(images),dtype=torch.long,device=device)\n",
        "                total_loss = (loss_img(logits_per_image,ground_truth) + loss_txt(logits_per_text,ground_truth))/2\n",
        "\n",
        "                val_losses += total_loss.item()\n",
        "                pbar_val.update(1)\n",
        "\n",
        "            # Get final validation loss for the epoch\n",
        "            final_val_loss=val_losses/len(val_dataloader)\n",
        "\n",
        "    # Print the losses\n",
        "    print(f'Epoch {epoch + 1} - Train Loss: {final_train_loss:.4f}, Validation Loss: {final_val_loss:.4f}')\n",
        "    train_loss_per_epoch.append(final_train_loss)\n",
        "    val_loss_per_epoch.append(final_val_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CsbqWgVODENI"
      },
      "source": [
        "Plot train and validation losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-bwFvCidDENI",
        "outputId": "f1a72ee3-9100-4436-b642-c9cf4e1bed3c"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(range(1, num_epochs+1), train_loss_per_epoch, label='Train Loss')\n",
        "plt.plot(range(1, num_epochs+1), val_loss_per_epoch, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dkGLACrOPLcU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S9Mb9YkaPLfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iIkPIgbnPLjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Yw1QtVR8PLl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QMyGmEXwPLoi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0ghhVewJPLrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Lstn_liuPLuf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yteTbzvcDENF"
      },
      "outputs": [],
      "source": [
        "# Example usage of get_image()\n",
        "id_to_image_mapping = {img_path.split('/')[1].split('.')[0]: get_image(img_path) for img_path in df['image_path']}\n",
        "df['image'] = df['article_id'].map(id_to_image_mapping)\n",
        "\n",
        "del id_to_image_mapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-Jv1E78DENG",
        "outputId": "08382c92-d0c3-462f-afb0-f0ddd5af9421"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>article_id</th>\n",
              "      <th>caption</th>\n",
              "      <th>image_path</th>\n",
              "      <th>image</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0108775015</td>\n",
              "      <td>womens Solid Black Vest top, Jersey top with n...</td>\n",
              "      <td>selected_images_1/0108775015.jpg</td>\n",
              "      <td>&lt;PIL.Image.Image image mode=RGB size=1166x1750...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0108775044</td>\n",
              "      <td>womens Solid White Vest top, Jersey top with n...</td>\n",
              "      <td>selected_images_1/0108775044.jpg</td>\n",
              "      <td>&lt;PIL.Image.Image image mode=RGB size=1166x1750...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0108775051</td>\n",
              "      <td>womens Stripe Off White Vest top, Jersey top w...</td>\n",
              "      <td>selected_images_1/0108775051.jpg</td>\n",
              "      <td>&lt;PIL.Image.Image image mode=RGB size=1166x1750...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0111586001</td>\n",
              "      <td>womens Solid Black Leggings, Tights with built...</td>\n",
              "      <td>selected_images_1/0111586001.jpg</td>\n",
              "      <td>&lt;PIL.Image.Image image mode=RGB size=1166x1750...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0116379047</td>\n",
              "      <td>womens Solid Dark Blue Top, Fitted top in soft...</td>\n",
              "      <td>selected_images_1/0116379047.jpg</td>\n",
              "      <td>&lt;PIL.Image.Image image mode=RGB size=1531x1750...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42236</th>\n",
              "      <td>0704630002</td>\n",
              "      <td>womens Solid Light Orange Sneakers, Trainers w...</td>\n",
              "      <td>selected_images_1/0704630002.jpg</td>\n",
              "      <td>&lt;PIL.Image.Image image mode=RGB size=1166x1750...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42237</th>\n",
              "      <td>0704630003</td>\n",
              "      <td>womens Colour blocking White Sneakers, Trainer...</td>\n",
              "      <td>selected_images_1/0704630003.jpg</td>\n",
              "      <td>&lt;PIL.Image.Image image mode=RGB size=1166x1750...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42238</th>\n",
              "      <td>0704630004</td>\n",
              "      <td>womens Colour blocking Black Sneakers, Trainer...</td>\n",
              "      <td>selected_images_1/0704630004.jpg</td>\n",
              "      <td>&lt;PIL.Image.Image image mode=RGB size=1166x1750...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42239</th>\n",
              "      <td>0704630005</td>\n",
              "      <td>womens Colour blocking Orange Sneakers, Traine...</td>\n",
              "      <td>selected_images_1/0704630005.jpg</td>\n",
              "      <td>&lt;PIL.Image.Image image mode=RGB size=1166x1750...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42240</th>\n",
              "      <td>0704630008</td>\n",
              "      <td>womens Colour blocking Black Sneakers, Trainer...</td>\n",
              "      <td>selected_images_1/0704630008.jpg</td>\n",
              "      <td>&lt;PIL.Image.Image image mode=RGB size=1166x1750...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>27284 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       article_id                                            caption  \\\n",
              "0      0108775015  womens Solid Black Vest top, Jersey top with n...   \n",
              "1      0108775044  womens Solid White Vest top, Jersey top with n...   \n",
              "2      0108775051  womens Stripe Off White Vest top, Jersey top w...   \n",
              "3      0111586001  womens Solid Black Leggings, Tights with built...   \n",
              "6      0116379047  womens Solid Dark Blue Top, Fitted top in soft...   \n",
              "...           ...                                                ...   \n",
              "42236  0704630002  womens Solid Light Orange Sneakers, Trainers w...   \n",
              "42237  0704630003  womens Colour blocking White Sneakers, Trainer...   \n",
              "42238  0704630004  womens Colour blocking Black Sneakers, Trainer...   \n",
              "42239  0704630005  womens Colour blocking Orange Sneakers, Traine...   \n",
              "42240  0704630008  womens Colour blocking Black Sneakers, Trainer...   \n",
              "\n",
              "                             image_path  \\\n",
              "0      selected_images_1/0108775015.jpg   \n",
              "1      selected_images_1/0108775044.jpg   \n",
              "2      selected_images_1/0108775051.jpg   \n",
              "3      selected_images_1/0111586001.jpg   \n",
              "6      selected_images_1/0116379047.jpg   \n",
              "...                                 ...   \n",
              "42236  selected_images_1/0704630002.jpg   \n",
              "42237  selected_images_1/0704630003.jpg   \n",
              "42238  selected_images_1/0704630004.jpg   \n",
              "42239  selected_images_1/0704630005.jpg   \n",
              "42240  selected_images_1/0704630008.jpg   \n",
              "\n",
              "                                                   image  \n",
              "0      <PIL.Image.Image image mode=RGB size=1166x1750...  \n",
              "1      <PIL.Image.Image image mode=RGB size=1166x1750...  \n",
              "2      <PIL.Image.Image image mode=RGB size=1166x1750...  \n",
              "3      <PIL.Image.Image image mode=RGB size=1166x1750...  \n",
              "6      <PIL.Image.Image image mode=RGB size=1531x1750...  \n",
              "...                                                  ...  \n",
              "42236  <PIL.Image.Image image mode=RGB size=1166x1750...  \n",
              "42237  <PIL.Image.Image image mode=RGB size=1166x1750...  \n",
              "42238  <PIL.Image.Image image mode=RGB size=1166x1750...  \n",
              "42239  <PIL.Image.Image image mode=RGB size=1166x1750...  \n",
              "42240  <PIL.Image.Image image mode=RGB size=1166x1750...  \n",
              "\n",
              "[27284 rows x 4 columns]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
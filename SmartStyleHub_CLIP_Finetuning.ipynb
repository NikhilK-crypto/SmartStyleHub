{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch==1.13.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import skimage\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import IPython.display\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "from collections import OrderedDict\n",
    "from transformers import CLIPProcessor, CLIPModel, CLIPTokenizer\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import clip\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the csv file containing selected articles after EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles=pd.read_csv('selected_articles.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the missing 0 to the article id and converting it to string as per the original dataset\n",
    "articles['article_id'] = articles['article_id'].astype(str)\n",
    "articles['article_id']=[\"0\"+x for x in articles['article_id']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the image path corresponding to each article id from zipped folders. We're adding the paths to the dataframe in a separate column.\n",
    "This will be used later to load the images during model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_file_path_1='H&M_dataset/selected_images_1.zip'\n",
    "zip_file_path_2='H&M_dataset/selected_images.zip'\n",
    "\n",
    "with zipfile.ZipFile(zip_file_path_1, 'r') as zip_ref:\n",
    "        files_in_zip_1 = zip_ref.namelist()\n",
    "        \n",
    "with zipfile.ZipFile(zip_file_path_2, 'r') as zip_ref:\n",
    "        files_in_zip_2 = zip_ref.namelist()\n",
    "\n",
    "df['image_path'] = None\n",
    "\n",
    "article_ids_1 = [file.split('/')[1].split('.')[0] for file in files_in_zip_1]\n",
    "article_ids_2 = [file.split('/')[1].split('.')[0] for file in files_in_zip_2]\n",
    "\n",
    "# Create a dictionary to map article IDs to file paths\n",
    "id_to_path_mapping_1 = {file.split('/')[1].split('.')[0]: file for file in files_in_zip_1}\n",
    "id_to_path_mapping_2 = {file.split('/')[1].split('.')[0]: file for file in files_in_zip_2}\n",
    "\n",
    "id_to_path_mapping_1.update(id_to_path_mapping_2)\n",
    "\n",
    "# Update the image_path column using vectorized operations\n",
    "df['image_path'] = df['article_id'].map(id_to_path_mapping_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>caption</th>\n",
       "      <th>image_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0108775015</td>\n",
       "      <td>womens Solid Black Vest top, Jersey top with n...</td>\n",
       "      <td>selected_images_1/0108775015.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0108775044</td>\n",
       "      <td>womens Solid White Vest top, Jersey top with n...</td>\n",
       "      <td>selected_images_1/0108775044.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0108775051</td>\n",
       "      <td>womens Stripe Off White Vest top, Jersey top w...</td>\n",
       "      <td>selected_images_1/0108775051.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0111586001</td>\n",
       "      <td>womens Solid Black Leggings, Tights with built...</td>\n",
       "      <td>selected_images_1/0111586001.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0116379047</td>\n",
       "      <td>womens Solid Dark Blue Top, Fitted top in soft...</td>\n",
       "      <td>selected_images_1/0116379047.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84371</th>\n",
       "      <td>0952937003</td>\n",
       "      <td>womens All over pattern Beige Dress, Fitted, c...</td>\n",
       "      <td>selected_images/0952937003.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84372</th>\n",
       "      <td>0952938001</td>\n",
       "      <td>womens All over pattern Beige Top, Fitted top ...</td>\n",
       "      <td>selected_images/0952938001.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84373</th>\n",
       "      <td>0953763001</td>\n",
       "      <td>womens Solid Black Vest top, Loose-fitting spo...</td>\n",
       "      <td>selected_images/0953763001.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84374</th>\n",
       "      <td>0956217002</td>\n",
       "      <td>womens Solid Black Dress, Short, A-line dress ...</td>\n",
       "      <td>selected_images/0956217002.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84375</th>\n",
       "      <td>0959461001</td>\n",
       "      <td>womens Solid Off White Dress, Calf-length dres...</td>\n",
       "      <td>selected_images/0959461001.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>55332 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       article_id                                            caption  \\\n",
       "0      0108775015  womens Solid Black Vest top, Jersey top with n...   \n",
       "1      0108775044  womens Solid White Vest top, Jersey top with n...   \n",
       "2      0108775051  womens Stripe Off White Vest top, Jersey top w...   \n",
       "3      0111586001  womens Solid Black Leggings, Tights with built...   \n",
       "6      0116379047  womens Solid Dark Blue Top, Fitted top in soft...   \n",
       "...           ...                                                ...   \n",
       "84371  0952937003  womens All over pattern Beige Dress, Fitted, c...   \n",
       "84372  0952938001  womens All over pattern Beige Top, Fitted top ...   \n",
       "84373  0953763001  womens Solid Black Vest top, Loose-fitting spo...   \n",
       "84374  0956217002  womens Solid Black Dress, Short, A-line dress ...   \n",
       "84375  0959461001  womens Solid Off White Dress, Calf-length dres...   \n",
       "\n",
       "                             image_path  \n",
       "0      selected_images_1/0108775015.jpg  \n",
       "1      selected_images_1/0108775044.jpg  \n",
       "2      selected_images_1/0108775051.jpg  \n",
       "3      selected_images_1/0111586001.jpg  \n",
       "6      selected_images_1/0116379047.jpg  \n",
       "...                                 ...  \n",
       "84371    selected_images/0952937003.jpg  \n",
       "84372    selected_images/0952938001.jpg  \n",
       "84373    selected_images/0953763001.jpg  \n",
       "84374    selected_images/0956217002.jpg  \n",
       "84375    selected_images/0959461001.jpg  \n",
       "\n",
       "[55332 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping NA values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting into train, validation and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_cleaned['image_path'], df_cleaned['caption'], test_size=0.2, random_state=1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1)\n",
    "\n",
    "train_data = pd.DataFrame({'image_path':X_train.values, 'caption': y_train.values})\n",
    "val_data = pd.DataFrame({'image_path':X_val.values, 'caption': y_val.values})\n",
    "test_data = pd.DataFrame({'image_path':X_test.values, 'caption': y_test.values})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a few functions and a class to load the CLIP model and prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_info(model_ID, device):\n",
    "    # Save the model to device\n",
    "    model = CLIPModel.from_pretrained(model_ID).to(device)\n",
    "    # Get the processor\n",
    "    processor = CLIPProcessor.from_pretrained(model_ID)\n",
    "    # Get the tokenizer\n",
    "    tokenizer = CLIPTokenizer.from_pretrained(model_ID)\n",
    "       # Return model, processor & tokenizer\n",
    "    return model, processor, tokenizer\n",
    "# Set the device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# Define the model ID\n",
    "model_ID = \"openai/clip-vit-base-patch32\"\n",
    "# Get model, processor & tokenizer\n",
    "model, processor, tokenizer = get_model_info(model_ID, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "z1 = zipfile.ZipFile('H&M_dataset/selected_images_1.zip')\n",
    "z2 = zipfile.ZipFile('H&M_dataset/selected_images.zip')\n",
    "\n",
    "def get_image(image_path_inside_zip):\n",
    "    if image_path_inside_zip in z1.namelist():\n",
    "        # Extract the specific image file\n",
    "        image = z1.read(image_path_inside_zip)\n",
    "\n",
    "    elif image_path_inside_zip in z2.namelist():\n",
    "        image = z2.read(image_path_inside_zip)\n",
    "        \n",
    "    #image = Image.open(BytesIO(image)).convert(\"RGB\")\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class image_title_dataset():\n",
    "    def __init__(self, list_image_path,list_txt):\n",
    "        # Initialize image paths and corresponding texts\n",
    "        self.image_path = list_image_path\n",
    "        self.title  = list_txt\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.title)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        return self.image_path[idx], self.title[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the dataset and create a dataloader object out of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use your own data\n",
    "list_image_path_train = []\n",
    "list_txt_train = []\n",
    "list_image_path_val = []\n",
    "list_txt_val = []\n",
    "\n",
    "for index,item in train_data.iterrows():\n",
    "    if index==500:\n",
    "        break\n",
    "    img_path = item['image_path'] #+ item['image_path'].split('/')[-1]\n",
    "    #print(img_path)\n",
    "    #print(item['caption'][:40])\n",
    "    caption = item['caption'][:77]\n",
    "    list_image_path_train.append(img_path)\n",
    "    list_txt_train.append(caption)\n",
    "    \n",
    "for index,item in val_data.iterrows():\n",
    "    if index==100:\n",
    "        break\n",
    "    img_path = item['image_path'] #+ item['image_path'].split('/')[-1]\n",
    "    #print(img_path)\n",
    "    caption = item['caption'][:77]\n",
    "    list_image_path_val.append(img_path)\n",
    "    list_txt_val.append(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = image_title_dataset(list_image_path_train, list_txt_train)\n",
    "val_dataset = image_title_dataset(list_image_path_val, list_txt_val)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=50, shuffle=True) #Define your own dataloader\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first_batch = next(iter(train_dataloader))\n",
    "\n",
    "# # Print the first batch\n",
    "# print(first_batch[0][0])\n",
    "# x=[]\n",
    "# x.append(Image.open(BytesIO(get_image(first_batch[0][0]))).convert(\"RGB\").resize((100, 100)))\n",
    "\n",
    "# next_batch = next(iter(train_dataloader))\n",
    "\n",
    "# x.append(Image.open(BytesIO(get_image(next_batch[0][0]))).convert(\"RGB\").resize((100, 100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to convert model's parameters to FP32 format\n",
    "def convert_models_to_fp32(model): \n",
    "    for p in model.parameters(): \n",
    "        p.data = p.data.float() \n",
    "        p.grad.data = p.grad.data.float() \n",
    "\n",
    "\n",
    "if device == \"cpu\":\n",
    "    model.float()\n",
    "\n",
    "# Prepare the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5,betas=(0.9,0.98),eps=1e-6,weight_decay=0.2) # the lr is smaller, more safe for fine tuning to new dataset\n",
    "\n",
    "\n",
    "# Specify the loss function\n",
    "loss_img = nn.CrossEntropyLoss()\n",
    "loss_txt = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [05:02<00:00, 30.29s/it]\n",
      "Epoch 1 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [01:00<00:00,  6.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Train Loss: 2.5610, Validation Loss: 1.1274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [05:02<00:00, 30.23s/it]\n",
      "Epoch 2 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [01:00<00:00,  6.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Train Loss: 2.1148, Validation Loss: 1.0099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [05:00<00:00, 30.09s/it]\n",
      "Epoch 3 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [01:00<00:00,  6.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Train Loss: 1.9075, Validation Loss: 0.8682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "num_epochs = 3\n",
    "train_loss_per_epoch=[]\n",
    "val_loss_per_epoch=[]\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_losses=0\n",
    "\n",
    "    with tqdm(total=len(train_dataloader), desc=f'Epoch {epoch + 1} - Training') as pbar_train:\n",
    "        for images, texts in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            #print(images)\n",
    "#             images=images.to(device)\n",
    "#             texts=texts.to(device)\n",
    "            #print(\"hi\")\n",
    "            #image_names=[print(img) for img in images]\n",
    "            #print(image_names)\n",
    "            #print(\"\\nhi\")\n",
    "            image_list=[Image.open(BytesIO(get_image(img))).convert(\"RGB\").resize((100, 100)) for img in images]\n",
    "            #print(image_list)\n",
    "            #image_list = [Image.open(BytesIO((requests.get(img)).content)).convert(\"RGB\") for img in images]\n",
    "            inputs = processor(text=texts, images=image_list, return_tensors=\"pt\", padding=True)\n",
    "            inputs=inputs.to(device)\n",
    "            #print(inputs)\n",
    "            outputs = model(**inputs)\n",
    "            logits_per_image, logits_per_text = outputs.logits_per_image, outputs.logits_per_text\n",
    "\n",
    "            # Compute loss\n",
    "            ground_truth = torch.arange(len(images),dtype=torch.long,device=device)\n",
    "            total_loss = (loss_img(logits_per_image,ground_truth) + loss_txt(logits_per_text,ground_truth))/2\n",
    "\n",
    "            # Backward pass\n",
    "            total_loss.backward()\n",
    "            if device == \"cpu\":\n",
    "                optimizer.step()\n",
    "                #optimizer.zero_grad()\n",
    "            else : \n",
    "                convert_models_to_fp32(model)\n",
    "                optimizer.step()\n",
    "                #optimizer.zero_grad()\n",
    "                #clip.model.convert_weights(model)\n",
    "            train_losses += total_loss.item()\n",
    "            pbar_train.update(1)\n",
    "            \n",
    "        final_train_loss=train_losses/len(train_dataloader)\n",
    "    \n",
    "    val_losses=0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with tqdm(total=len(val_dataloader), desc=f'Epoch {epoch + 1} - Validation') as pbar_val:\n",
    "            for images, texts in val_dataloader:\n",
    "\n",
    "                image_list=[Image.open(BytesIO(get_image(img))).convert(\"RGB\").resize((100, 100)) for img in images]\n",
    "                #image_list = [Image.open(BytesIO((requests.get(img)).content)).convert(\"RGB\") for img in images]\n",
    "\n",
    "                inputs = processor(text=texts, images=image_list, return_tensors=\"pt\", padding=True)\n",
    "                \n",
    "                inputs=inputs.to(device)\n",
    "                outputs = model(**inputs)\n",
    "                logits_per_image, logits_per_text = outputs.logits_per_image, outputs.logits_per_text\n",
    "\n",
    "                # Compute loss\n",
    "                ground_truth = torch.arange(len(images),dtype=torch.long,device=device)\n",
    "                total_loss = (loss_img(logits_per_image,ground_truth) + loss_txt(logits_per_text,ground_truth))/2\n",
    "\n",
    "                val_losses += total_loss.item()\n",
    "                pbar_val.update(1)\n",
    "                \n",
    "            final_val_loss=val_losses/len(val_dataloader)\n",
    "\n",
    "    print(f'Epoch {epoch + 1} - Train Loss: {final_train_loss:.4f}, Validation Loss: {final_val_loss:.4f}')\n",
    "    train_loss_per_epoch.append(final_train_loss)\n",
    "    val_loss_per_epoch.append(final_val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot train and validation losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(1, num_epochs+1), train_loss_per_epoch, label='Train Loss')\n",
    "plt.plot(range(1, num_epochs+1), val_loss_per_epoch, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

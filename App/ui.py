import streamlit as st
from llama2_prompt import *
from transformers import CLIPModel, CLIPProcessor, CLIPTokenizer
import torch
from qdrant_client import QdrantClient
import pandas as pd 
from PIL import Image



## getting the dataframe 
#-----------------------
df = pd.read_csv('final_selected_articles_df.csv')



## model Loading section 
#-----------------------
device = torch.device("cpu")

model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")# Initialize the model architecture
# Load the fine-tuned model parameters
model_file = torch.load('/Users/nikhilkudupudi/Projects - School & Work/Capstone/SmartStyleHub/fine_tuned_model/CLIP_finetuned_no_detail_desc.pth', map_location=device)
model.load_state_dict(model_file['model_state_dict'])
model.to(device)  # Move model to the appropriate device (CPU or GPU)

# Loading the processor and tokenizer
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
tokenizer = CLIPTokenizer.from_pretrained("openai/clip-vit-base-patch32")



## connecting to the vector database
#----------------------------------
try:
    qdrant_client = QdrantClient(
        url="https://5647fd82-90a5-4a20-b099-68762d7b46ce.us-east4-0.gcp.cloud.qdrant.io:6333", 
        api_key="Nz75e1xi7_YvMmO7BImqId2fNMMQjXPfhJwEwT_HTMOwPNPmqruAUw",
    )
    # print("client created successfully")

except QuadrantAPIError as e:
    # Handle API errors, if any
    print("Error creating Quadrant client:", e)
except Exception as e:
    # Handle any other unexpected errors
    print("An unexpected error occurred:", e)



## converting text to embeddings using model and returing 
#--------------------------------------------------------
def text_to_image_search(response, gender):
    multi_hits = []

    for text in response:
        if gender == 'Men':
            text = text + ' for male' 

        else:
            text = 'womens ' + text 

        inp = tokenizer(text, return_tensors="pt")
        text_embeddings = model.get_text_features(**inp).cpu().detach().numpy().tolist()[0]
        hits = qdrant_client.search(
            collection_name="image_embeddings_by_finetuned_clip",
            query_vector=("image",text_embeddings),
            limit=1,
            with_vectors=True
        )

        multi_hits.append(hits)

    return multi_hits



def display_image_results(search_results):
    # Resize images and gather captions
    images_and_captions = [(Image.open(df.loc[df.index == result[0].id, 'article_id'].values[0]).resize((800, 800)), 
                            result[0].payload['caption']) for result in search_results]
    
    # Calculate the number of rows needed to display all images in groups of 3
    num_rows = len(images_and_captions) // 3 + (1 if len(images_and_captions) % 3 > 0 else 0)
    
    for i in range(num_rows):
        # Extract 3 images and captions for the current row
        row_images_and_captions = images_and_captions[i*3: (i+1)*3]
        
        # Create a row of columns for the images
        cols = st.columns(3)
        
        # Iterate over each column in the current row and display the image with caption
        for col, (image, caption) in zip(cols, row_images_and_captions):
            with col:
                st.image(image, width=150, caption=caption)

# Call the function with your search_results data
# display_image_results(search_results)


## Main block 
#------------

def main():

    st.title("SmartStyleHub")
    
    # Gender selection dropdown
    gender = st.selectbox("Select the Attire Section", ["Men", "Women"])
    ocassion= st.selectbox("Select the ocassion", ["Interview","Party","Workout","Date","Wedding Ceremony"])
    
    # Text input for user message
    # user_message = st.text_input("What is the ocassion?:")
    other_option = st.checkbox("Other")
    
    # Text input for user message if 'Other' option is selected
    if other_option:
        ocassion = st.text_input("Please specify what is the ocassion?:", "")
    
    # Button to send message
    if st.button("Submit"):

        llama_response = call_llama(ocassion,gender)
        response = sum(clean_tokens(llama_response) , [])

        print(response)

        display_image_results(text_to_image_search(response, gender))
            
        st.write('response generated by chat model - ')
        st.write(llama_response)
                

if __name__ == "__main__":
    main()
